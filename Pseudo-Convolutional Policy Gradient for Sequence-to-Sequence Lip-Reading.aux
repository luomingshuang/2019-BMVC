\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{B2017}
\citation{Chung}
\citation{Zhao2009}
\citation{Zhao2015}
\citation{cooke2006}
\citation{B2017}
\citation{Yang2019}
\citation{Chung}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{Rohit}
\citation{IIya}
\citation{Chen2017}
\citation{Venugopalan}
\citation{Chopra2016}
\citation{Rennie}
\citation{Chopra2016}
\citation{Chopra2016}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Video encoder.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{figure1}{{1}{2}{Video encoder.\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Our sequence-to-sequence lip-reading model architecture. The input is sequence frames and the output is sequence characters. The seq2seq model generates the target character by character. For example, the model generates ("a","b","o","u","t") successively when the ground truth is the word "about". The classifier can work on word-level lip-reading datasets. We utilize Monte Carlo sampling to sample M transcription sequences from our model to calculate the policy gradient.\relax }}{2}{figure.caption.2}}
\newlabel{figure2}{{2}{2}{Our sequence-to-sequence lip-reading model architecture. The input is sequence frames and the output is sequence characters. The seq2seq model generates the target character by character. For example, the model generates ("a","b","o","u","t") successively when the ground truth is the word "about". The classifier can work on word-level lip-reading datasets. We utilize Monte Carlo sampling to sample M transcription sequences from our model to calculate the policy gradient.\relax }{figure.caption.2}{}}
\citation{Afouras}
\citation{Assael2016}
\citation{B2017}
\citation{Chung}
\citation{Chung2017}
\citation{Chung2018}
\citation{Petridis2018}
\citation{Yang2019}
\citation{Zhou2014}
\citation{Afouras}
\citation{Graves2006}
\citation{Assael2016}
\citation{Assael2016}
\citation{Afouras}
\citation{Wiseman2016}
\citation{Chung}
\citation{HIWTC2014}
\citation{Parikh2016}
\citation{Vaswani2017}
\citation{Xu2014}
\citation{Chung2017}
\citation{Petridis2018}
\citation{Papineni2002}
\citation{Lin2001}
\citation{Banerjee2003}
\citation{Tech}
\citation{Chen2014}
\citation{Chopra2016}
\citation{Li2018}
\citation{McConnell2002}
\citation{Pasunuru2017}
\citation{Ren2017}
\citation{Rennie}
\citation{Shen}
\citation{Tjandra2017}
\citation{Wang}
\citation{Tjandra2017}
\citation{Rennie}
\citation{Chopra2016}
\citation{Willia1992}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{3}{section.2}}
\citation{Willia1992}
\@writefile{toc}{\contentsline {section}{\numberline {3}The Proposed work}{4}{section.3}}
\newlabel{sec:Pro}{{3}{4}{The Proposed work}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model architecture}{4}{subsection.3.1}}
\newlabel{section3.1}{{3.1}{4}{Model architecture}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}REINFORCE algorithm}{5}{subsection.3.2}}
\newlabel{section3.2}{{3.2}{5}{REINFORCE algorithm}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Reward function $\&$ PCPG (Pseudo-Convolutional Policy Gradient)}{5}{subsection.3.3}}
\newlabel{section3.3}{{3.3}{5}{Reward function $\&$ PCPG (Pseudo-Convolutional Policy Gradient)}{subsection.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Pseudo-convolutional Policy Gradient. In this figure, we set the kernel size $k$ to 5, the stride $s$ to 1, and the kernel weights $w$ to [$\genfrac  {}{}{}0{1}{5}$,$\genfrac  {}{}{}0{1}{5}$,$\genfrac  {}{}{}0{1}{5}$,$\genfrac  {}{}{}0{1}{5}$,$\genfrac  {}{}{}0{1}{5}$].\relax }}{6}{figure.caption.3}}
\newlabel{figure3}{{3}{6}{Pseudo-convolutional Policy Gradient. In this figure, we set the kernel size $k$ to 5, the stride $s$ to 1, and the kernel weights $w$ to [$\dfrac {1}{5}$,$\dfrac {1}{5}$,$\dfrac {1}{5}$,$\dfrac {1}{5}$,$\dfrac {1}{5}$].\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{7}{section.4}}
\newlabel{section4}{{4}{7}{Experiments}{section.4}{}}
\citation{Assael2016}
\citation{Yang2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Ablation study}{8}{subsection.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The experimental results on GRID. Accuracy=1-WER. (\textbf  {RF}: receptive field, \textbf  {OP}: overlapping parts) \vspace  {-0.2cm}\relax }}{8}{table.caption.4}}
\newlabel{table1}{{1}{8}{The experimental results on GRID. Accuracy=1-WER. (\textbf {RF}: receptive field, \textbf {OP}: overlapping parts) \vspace {-0.2cm}\relax }{table.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Acc-epoch on GRID.\relax }}{8}{figure.caption.5}}
\newlabel{figure5}{{4}{8}{Acc-epoch on GRID.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Acc-epoch on LRW.\relax }}{8}{figure.caption.5}}
\newlabel{figure6}{{5}{8}{Acc-epoch on LRW.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Acc-epoch on LRW-1000.\relax }}{8}{figure.caption.5}}
\newlabel{figure7}{{6}{8}{Acc-epoch on LRW-1000.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces $L_{PCPG}$-iteration on GRID.\relax }}{8}{figure.caption.6}}
\newlabel{figure8}{{7}{8}{$L_{PCPG}$-iteration on GRID.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces $L_{PCPG}$-iteration on LRW.\relax }}{8}{figure.caption.6}}
\newlabel{figure9}{{8}{8}{$L_{PCPG}$-iteration on LRW.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces $L_{PCPG}$-iteration on LRW-1000.\relax }}{8}{figure.caption.6}}
\newlabel{figure10}{{9}{8}{$L_{PCPG}$-iteration on LRW-1000.\relax }{figure.caption.6}{}}
\bibstyle{bmvc}
\bibdata{egbib}
\bibcite{Afouras}{{1}{2018}{{Afouras et~al.}}{{Afouras, Chung, and Zisserman}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Evaluation of the kernel size $k$ in PCPG}{9}{subsection.4.2}}
\newlabel{section4.2}{{4.2}{9}{Evaluation of the kernel size $k$ in PCPG}{subsection.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The experimental results when model has different $k$. ($s$=1)\relax }}{9}{table.caption.7}}
\newlabel{table2}{{2}{9}{The experimental results when model has different $k$. ($s$=1)\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evaluation of the kernel weight $w$ in PCPG}{9}{subsection.4.3}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The experiments'results when model has different $w$. ($k$=3, $s$=1)\relax }}{9}{table.caption.8}}
\newlabel{table3}{{3}{9}{The experiments'results when model has different $w$. ($k$=3, $s$=1)\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{9}{section.5}}
\bibcite{Assael2016}{{2}{2016}{{Assael et~al.}}{{Assael, Shillingford, Whiteson, and Freitas}}}
\bibcite{B2017}{{3}{2017}{{B and Zisserman}}{{}}}
\bibcite{Banerjee2003}{{4}{2005}{{Banerjee and Lavie}}{{}}}
\bibcite{Chen2014}{{5}{2014}{{Chen and Steinberger}}{{}}}
\bibcite{Chen2017}{{6}{2017}{{Chen et~al.}}{{Chen, Liao, Chuang, Hsu, Fu, and Sun}}}
\bibcite{Chopra2016}{{7}{2016}{{Chopra et~al.}}{{Chopra, Auli, and Zaremba}}}
\bibcite{Chung}{{8}{2017}{{Chung}}{{}}}
\bibcite{Chung2017}{{9}{2017}{{Chung and Zisserman}}{{}}}
\bibcite{Chung2018}{{10}{2018}{{Chung and Zisserman}}{{}}}
\bibcite{cooke2006}{{11}{2006}{{Cooke}}{{}}}
\bibcite{Graves2006}{{12}{2006}{{Graves and Fern}}{{}}}
\bibcite{Zhao2015}{{13}{2015}{{Iryna~Anina and PietikÂ¨ainen}}{{}}}
\bibcite{HIWTC2014}{{14}{2014}{{Jianlong~Fu}}{{}}}
\bibcite{Li2018}{{15}{2018}{{Li and Gong}}{{}}}
\bibcite{Lin2001}{{16}{2004}{{Lin and Rey}}{{}}}
\bibcite{McConnell2002}{{17}{2002}{{McConnell et~al.}}{{McConnell, Berhane, Gilliland, London, Islam, Gauderman, Avol, Margolis, and Peters}}}
\bibcite{Papineni2002}{{18}{2002}{{Papineni et~al.}}{{Papineni, Roukos, Ward, and Zhu}}}
\bibcite{IIya}{{19}{2014}{{Parikh and Uszkoreit}}{{}}}
\bibcite{Parikh2016}{{20}{2016}{{Parikh and Uszkoreit}}{{}}}
\bibcite{Pasunuru2017}{{21}{2017}{{Pasunuru and Bansal}}{{}}}
\bibcite{Petridis2018}{{22}{2018}{{Petridis et~al.}}{{Petridis, Stafylakis, Ma, and Cai}}}
\bibcite{Ren2017}{{23}{2017}{{Ren et~al.}}{{Ren, Wang, and Zhang}}}
\bibcite{Rennie}{{24}{2017}{{Rennie et~al.}}{{Rennie, Marcheret, Mroueh, Ross, and Goel}}}
\bibcite{Rohit}{{25}{2017}{{Rohit~Prabhavalkar}}{{}}}
\bibcite{Shen}{{26}{2019}{{Shen et~al.}}{{Shen, Huang, Wang, Tsao, Wang, Chi, Engineering, and Chiao}}}
\bibcite{Tech}{{27}{2015}{{Tech et~al.}}{{Tech, Zitnick, and Parikh}}}
\bibcite{Tjandra2017}{{28}{2018}{{Tjandra et~al.}}{{Tjandra, Sakti, and Nakamura}}}
\bibcite{Vaswani2017}{{29}{2017}{{Vaswani}}{{}}}
\bibcite{Venugopalan}{{30}{2015}{{Venugopalan et~al.}}{{Venugopalan, Rohrbach, Darrell, Donahue, Saenko, and Mooney}}}
\bibcite{Wang}{{31}{2018}{{Wang et~al.}}{{Wang, Chen, Wu, and Wang}}}
\bibcite{Willia1992}{{32}{1992}{{Willia}}{{}}}
\bibcite{Wiseman2016}{{33}{2016}{{Wiseman and Rush}}{{}}}
\bibcite{Xu2014}{{34}{2015}{{Xu et~al.}}{{Xu, Courville, Zemel, and Bengio}}}
\bibcite{Yang2019}{{35}{2018}{{Yang et~al.}}{{Yang, Zhang, Feng, Yang, Wang, Xiao, Long, Shan, and Chen}}}
\bibcite{Zhao2009}{{36}{2009}{{Zhao et~al.}}{{Zhao, Barnard, and Pietik{\"{a}}inen}}}
\bibcite{Zhou2014}{{37}{2014}{{Zhou et~al.}}{{Zhou, Zhao, Hong, and Pietik{\"{a}}inen}}}
\gdef \BMVA@LastPage{12}
